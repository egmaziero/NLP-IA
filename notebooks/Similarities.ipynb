{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Overlap\n",
    "Simple way to measure lexical overlaping in two texts (sentence, paragraph or document).\n",
    "It removes punctuation and stopwords. Then, calculates the amount of common words over the sum of the words in two texts. So, this measure goes from 0 (different) to 0.5 (equal)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords_list = nltk.corpus.stopwords.words('portuguese')\n",
    "\n",
    "def tokenize_words(t):\n",
    "    return nltk.tokenize.word_tokenize(t)\n",
    "\n",
    "def remove_stopwords(tokens):\n",
    "    return [t for t in tokens if t not in stopwords_list]\n",
    "\n",
    "def remove_punctuation(t):\n",
    "    return t.translate(str.maketrans('','',string.punctuation))\n",
    "\n",
    "def word_overlap(t1, t2):\n",
    "    tokens1 = remove_stopwords(tokenize_words(remove_punctuation(t1)))\n",
    "    tokens2 = remove_stopwords(tokenize_words(remove_punctuation(t2)))\n",
    "    matches = [1 for t in tokens1 if t in tokens2]\n",
    "    return sum(matches) / (len(tokens1) + len(tokens2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t1 = \"Contagem manual de milhões de votos deixa 272 pessoas mortas e outras 1.878 doentes na Indonésia\"\n",
    "word_overlap(t1, t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t2 = \"Primeira eleição que juntou o voto presidencial com parlamentares nacionais e regionais num mesmo pleito.\"\n",
    "word_overlap(t1, t2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4166666666666667"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t3 = \"272 pessoas mortas e outras 1.878 doentes na Indonésia após contagem manual de milhões de votos.\"\n",
    "word_overlap(t1, t3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec\n",
    "download a pretrained wordembedding at http://www.nilc.icmc.usp.br/nilc/index.php/repositorio-de-word-embeddings-do-nilc\n",
    "\n",
    "You can click in the following link: http://143.107.183.175:22980/download.php?file=embeddings/word2vec/skip_s50.zip. Unzip the file inside notebooks directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "model = KeyedVectors.load_word2vec_format('skip_s50.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/erickmaziero/virtualenvs/NLP-IA_env/lib/python3.7/site-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
      "  if np.issubdtype(vec.dtype, np.int):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('passageiro', 0.907621443271637),\n",
       " ('trator', 0.9069172143936157),\n",
       " ('caminhão', 0.8961774706840515),\n",
       " ('jipe', 0.889147162437439),\n",
       " ('avião', 0.8786295652389526),\n",
       " ('guindaste', 0.8719200491905212),\n",
       " ('contêiner', 0.8684994578361511),\n",
       " ('parabrisa', 0.866083562374115),\n",
       " ('elevador', 0.8559513688087463),\n",
       " ('motorista', 0.8482216596603394)]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar('carro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word Embedding method with a cosine distance asses that our two sentences are similar to 100.0 %\n",
      "Word Embedding method with a cosine distance asses that our two sentences are similar to 46.93 %\n",
      "Word Embedding method with a cosine distance asses that our two sentences are similar to 47.78 %\n"
     ]
    }
   ],
   "source": [
    "import scipy\n",
    "import numpy as np\n",
    "def cosine_distance_wordembedding_method(t1, t2):\n",
    "    vector_1 = np.mean([model.get_vector(word) for word in t1.split() if word in model.vocab],axis=0)\n",
    "    vector_2 = np.mean([model.get_vector(word) for word in t2.split() if word in model.vocab],axis=0)\n",
    "    cosine = scipy.spatial.distance.cosine(vector_1, vector_2)\n",
    "    print('Word Embedding method with a cosine distance asses that our two sentences are similar to',round((1-cosine)*100,2),'%')\n",
    "\n",
    "cosine_distance_wordembedding_method(t1, t1)\n",
    "cosine_distance_wordembedding_method(t1, t2)\n",
    "cosine_distance_wordembedding_method(t2, t3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wordnets\n",
    "Do not forget to donwload: ```nltk.download()```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 -> a member of the genus Canis (probably descended from the common wolf) that has been domesticated by man since prehistoric times; occurs in many breeds\n",
      "1 -> a dull unattractive unpleasant girl or woman\n",
      "2 -> informal term for a man\n",
      "3 -> someone who is morally reprehensible\n",
      "4 -> a smooth-textured sausage of minced beef or pork usually smoked; often served on a bread roll\n",
      "5 -> a hinged catch that fits into a notch of a ratchet to move a wheel forward or prevent it from moving backward\n",
      "6 -> metal supports for logs in a fireplace\n",
      "7 -> go after with the intent to catch\n"
     ]
    }
   ],
   "source": [
    "dog_synsets = wordnet.synsets('dog')\n",
    "for i, dog in enumerate(dog_synsets):\n",
    "    print('{} -> {}'.format(i, dog.definition()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Synset('carnivore.n.01')]\n",
      "[Synset('vertebrate.n.01')]\n"
     ]
    }
   ],
   "source": [
    "print(wordnet.synset('dog.n.01').lowest_common_hypernyms(wordnet.synset('cat.n.01')))\n",
    "print(wordnet.synset('dog.n.01').lowest_common_hypernyms(wordnet.synset('duck.n.01')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### see http://www.nltk.org/howto/wordnet.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path similarity\n",
      "0.2\n",
      "0.07692307692307693\n",
      "\n",
      "Leacock-Chodorow similarity\n",
      "2.0281482472922856\n",
      "1.072636802264849\n",
      "\n",
      "Wu-Palmer similarity\n",
      "0.8571428571428571\n",
      "0.4\n"
     ]
    }
   ],
   "source": [
    "dog = wordnet.synset('dog.n.01')\n",
    "cat = wordnet.synset('cat.n.01')\n",
    "car = wordnet.synset('car.n.01')\n",
    "\n",
    "print('Path similarity')\n",
    "print(dog.path_similarity(cat))\n",
    "print(dog.path_similarity(car))\n",
    "print()\n",
    "print('Leacock-Chodorow similarity')\n",
    "print(dog.lch_similarity(cat))\n",
    "print(dog.lch_similarity(car))\n",
    "print()\n",
    "print('Wu-Palmer similarity')\n",
    "print(dog.wup_similarity(cat))\n",
    "print(dog.wup_similarity(car))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synset('entity.n.01') []\n",
      "Synset('physical_entity.n.01') [Synset('entity.n.01')]\n",
      "Synset('abstraction.n.06') [Synset('entity.n.01')]\n",
      "Synset('thing.n.12') [Synset('physical_entity.n.01')]\n",
      "Synset('object.n.01') [Synset('physical_entity.n.01')]\n",
      "Synset('whole.n.02') [Synset('object.n.01')]\n",
      "Synset('congener.n.03') [Synset('whole.n.02')]\n",
      "Synset('living_thing.n.01') [Synset('whole.n.02')]\n",
      "Synset('organism.n.01') [Synset('living_thing.n.01')]\n",
      "Synset('benthos.n.02') [Synset('organism.n.01')]\n"
     ]
    }
   ],
   "source": [
    "from itertools import islice\n",
    "for synset in islice(wordnet.all_synsets('n'), 10):\n",
    "    print(synset, synset.hypernyms())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Portuguese usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Synset('beach_wagon.n.01'), Synset('car.n.01'), Synset('car.n.02'), Synset('cart.n.01')]\n",
      "Synset('motor_vehicle.n.01')\n",
      "['veículos_motorizados']\n",
      "Synset('self-propelled_vehicle.n.01')\n",
      "[]\n",
      "Synset('wheeled_vehicle.n.01')\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "carro = wordnet.synsets('carro', lang='por')\n",
    "print(carro)\n",
    "# here a Word Sense Disambiguation tool is needed!\n",
    "\n",
    "hyper = carro[1].hypernyms()[0]\n",
    "print(hyper)\n",
    "print(hyper.lemma_names(lang='por'))\n",
    "hyper_hyper = hyper.hypernyms()[0]\n",
    "\n",
    "print(hyper_hyper)\n",
    "print(hyper_hyper.lemma_names(lang='por'))\n",
    "\n",
    "hyper_hyper_hyper = hyper_hyper.hypernyms()[0]\n",
    "print(hyper_hyper_hyper)\n",
    "print(hyper_hyper_hyper.lemma_names(lang='por'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Synsets\n",
    "Download synset database from http://www.nilc.icmc.usp.br/tep2/download.htm\n",
    "The file ```base_tep2.txt``` follows the format:\n",
    "\n",
    "Numero_sequencial do synset. \\[nome_da_categoria_do_synset\\] {Conjunto Synset separado por virgulas} <numero_identificador do(s) Synset(s) antonimo(s)>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "synsets = {}\n",
    "with open('base_tep2/base_tep2.txt', 'r', encoding='latin1') as tep_file:\n",
    "    for line in tep_file.readlines():\n",
    "        syn_id = re.match('^(?P<int>\\d+)', line).group('int')\n",
    "        cat = re.search('\\[(?P<cat>(.*?))\\]', line).group('cat')\n",
    "        syn = re.search('\\{(?P<syn>(.*?))\\}', line).group('syn')\n",
    "        s = re.search('\\<(?P<ant>(.*?))\\>', line)\n",
    "        ant = None\n",
    "        if s is not None:\n",
    "            ant = s.group('ant')\n",
    "        synsets[syn_id] = {'words': [w.strip() for w in syn.split(',')],\n",
    "                           'category': cat,\n",
    "                           'antonym': ant}\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
